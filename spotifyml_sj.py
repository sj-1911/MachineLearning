# -*- coding: utf-8 -*-
"""SpotifyML_SJ

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N3Wh-vH3SdAKOIDuUL-h5l7rOeMSfRVg
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam, SGD
import matplotlib.pyplot as plt

# Load dataset
file_path = r'C:\Users\spenc\Downloads\spotify_preprocessed.csv'
data = pd.read_csv(file_path)

# Prepare data
X = data.drop(columns=['target'])
y = data['target']

# Split data into train, validation, and test sets
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.1, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)

# Scale data
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Build and compile the model
model = Sequential([
    Dense(32, activation='relu', input_shape=(X_train_scaled.shape[1],)),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])

optimizer = Adam(learning_rate=0.01)
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train_scaled, y_train, epochs=50, batch_size=16, validation_data=(X_val_scaled, y_val))

# Evaluate model
test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)
print(f"Test Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")

# Plot training and validation loss
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()

# Hyperparameter tuning
num_hidden_layers_list = [1, 2, 3]
num_units_list = [16, 32, 64]
learning_rates = [0.001, 0.01, 0.1]
batch_sizes = [16, 32, 64]
epochs = 50

best_val_loss = float('inf')
best_hyperparameters = {}

for num_hidden_layers in num_hidden_layers_list:
    for num_units in num_units_list:
        for learning_rate in learning_rates:
            for batch_size in batch_sizes:
                model = Sequential()
                model.add(Dense(num_units, activation='relu', input_shape=(X_train_scaled.shape[1],)))
                for _ in range(num_hidden_layers - 1):
                    model.add(Dense(num_units, activation='relu'))
                model.add(Dense(1, activation='sigmoid'))

                optimizer = Adam(learning_rate=learning_rate)
                model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

                history = model.fit(X_train_scaled, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val_scaled, y_val), verbose=0)

                val_loss = model.evaluate(X_val_scaled, y_val, verbose=0)[0]

                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    best_hyperparameters = {
                        'num_hidden_layers': num_hidden_layers,
                        'num_units': num_units,
                        'learning_rate': learning_rate,
                        'batch_size': batch_size
                    }

print(f"Best Validation Loss: {best_val_loss:.4f}")
print(f"Best Hyperparameters: {best_hyperparameters}")